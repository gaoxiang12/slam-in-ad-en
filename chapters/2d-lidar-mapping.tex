% !Mode:: "TeX:UTF-8"
\thispagestyle{empty}  
\chapter{2D Laser Localization and Mapping}  
\thispagestyle{empty}  
\label{cpt:2d-mapping}  
In the previous chapter, we introduced the basic principles of laser measurement, as well as the nearest neighbor and fitting methods for point clouds. These serve as the foundation for most laser point cloud registration methods. However, people often treat 2D and 3D laser processing scenarios differently. Overall, 2D laser registration is easier and more suitable for introducing image-like processing methods. This chapter will introduce the relatively simpler 2D laser SLAM, while the next chapter will cover 3D laser SLAM systems. Readers can compare the differences between the two systems from theoretical and methodological perspectives.  

\includepdf[width=\textwidth]{art/ch6.pdf}

\section{Basic Principles of 2D Laser SLAM}

\begin{figure}[!htp]  
	\centering  
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/2d-robots.pdf}  
	\caption{Robots using 2D laser SLAM and their corresponding lidars. Cleaning robots typically mount lidars on top, while service robots have built-in lidars after slotting at the base.}  
	\label{fig:2d-robots}  
\end{figure}  

All real-world sensors naturally operate in three-dimensional space, inherently without any distinction of dimensionality. However, most wheeled robots move only on a fixed plane, unlike aircraft that freely change their posture. Cleaning robots operate on horizontal ground, while wall-climbing robots work on vertical planes. Some robots, such as hotel food delivery robots, may have a certain height in their main body, but the part primarily responsible for movement—and thus the main focus of SLAM algorithms—is two-dimensional, as shown in Figure~\ref{fig:2d-robots}.  

Compared to point clouds in three-dimensional space, 2D SLAM can be viewed as a laser SLAM algorithm operating from a top-down perspective. From this viewpoint, laser scan data and map data can be simplified into two-dimensional forms. They closely resemble images, and the map itself can even be stored as an image. Some image feature extraction and matching algorithms can also be applied to 2D SLAM. 2D SLAM is crucial for applications like cleaning robots and AGVs (Automated Guided Vehicles) and was once the dominant focus of SLAM technology \cite{Thrun2005} (it remains the most widely deployed field today). Historically, it has given rise to many well-known methods, such as FastSLAM \cite{Montemerlo2002}, GMapping \cite{Grisetti2007a}, and others.  

However, due to the assumption of planar motion, when the robot body or the environment contains significant three-dimensional objects, some fundamentally unsolvable problems arise at the system level. For example, most 2D SLAM solutions assume obstacles are at the same height as the laser sensor. If the environment contains obstacles at other heights or objects whose shapes vary noticeably with height (e.g., a tabletop and its legs are clearly different), 2D maps struggle to represent such objects, and the robot may collide with them. Another example is when the robot moves on an inclined slope, where the scanned object distances differ geometrically from the actual distances. These scenarios violate the 2D motion assumption and are inherent limitations of the system, making them difficult to resolve within the framework of 2D SLAM. The 3D point cloud SLAM introduced in the next chapter can effectively address these shortcomings caused by 2D assumptions.  

On the other hand, early 2D SLAM systems often treated the map as a single 2D image, an approach that was simplistic and not well-suited for handling loop closures. This chapter will introduce 2D SLAM from a more modern perspective, adopting a framework similar to that of 3D laser SLAM. The content arrangement will also emphasize the similarities between the two.  

\begin{figure}[!htp]  
	\centering  
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/2d-slam-pipeline.pdf}  
	\caption{The basic pipeline of submap-based 2D SLAM.}  
	\label{fig:2d-slam-pipeline}  
\end{figure}  

Figure~\ref{fig:2d-slam-pipeline}~illustrates a typical 2D SLAM framework. Here is a brief overview of its workflow:  

\begin{enumerate}  
	\item First, the 2D laser sensor outputs range measurements at a fixed frequency. Each full cycle of data is called a \textbf{scan} \footnote{Hereafter, the term "scan" will refer to the laser scan data within one cycle. Since terms like "scan-to-scan" are already widely used in the industry, we will not deliberately translate "scan."}.  
	\item To estimate the robot's pose for this scan, we need to \textbf{match} (or \textbf{register}) it against something. This process is called \textbf{scan matching}. We can match the scan either against the previous scan or against the map, so scan matching can be further divided into \textit{scan-to-scan} and \textit{scan-to-map} modes. The principles are largely the same, and they can be used flexibly in practice. In this chapter, we will implement common 2D scan matching algorithms, such as point-to-point and point-to-line methods.  
	\item After estimating the pose of this scan, we integrate it into the map. Of course, a scan is essentially a point cloud, so the simplest approach is to place all scans into the map in chronological order. However, this may suffer from cumulative errors or moving objects. Modern SLAM solutions often adopt a more flexible \textbf{submap} approach, grouping nearby laser scans into a submap and then stitching the submaps together \cite{Hess2016}. In the submap model, each submap is internally fixed and does not require repeated computation. At the same time, submaps have their own independent coordinate systems, and the poses between them can be adjusted and optimized. Thus, when handling loop closures, submaps can be treated as basic units. Early SLAM solutions often relied on a single global map \cite{Grisetti2007a}. Submaps represent an intermediate management approach between single frames and a full map, making loop closure detection and map updates more convenient. This chapter will also adopt the submap model for map construction.  
	\item Finally, how should the scanned map be stored and updated? Many robot maps need to distinguish between \textbf{obstacles} and \textbf{navigable areas}. To represent these concepts, we will use an \textbf{occupancy grid map} for map management \cite{Thrun2003a, MeyerDelius2012}. Occupancy grid maps can effectively filter out the impact of moving objects, resulting in cleaner maps.  
\end{enumerate}  

In this chapter, we will work with readers to implement the mainstream 2D SLAM algorithms discussed above. We will implement several key scan matching algorithms, build them into local submaps, and then use loop closure corrections to construct a complete occupancy grid map. Among the algorithms mentioned here, scan matching is the core of many subsequent processes. We can use traditional methods like the Iterative Closest Point (ICP) algorithm for scan matching or leverage the characteristics of 2D to implement image-based methods such as Gaussian likelihood fields.

\section{Scan Matching Algorithms}  
\subsection{Point-to-Point Scan Matching}  

Let us begin by introducing the scan matching methods in 2D SLAM. A single 2D scan is represented by a set of angle-distance pairs, denoted as $(\rho, r)_i$, where $\rho$ is the angle relative to the robot's own frame, $r$ is the measured distance, and $i = 0, \ldots N$ indicates multiple measurement points. The value of $N$ depends on the angular resolution of the laser sensor. In implementation, these data points are often stored in an array. These measurements are in polar coordinates and can be naturally converted to Cartesian coordinates, expressed as $(x,y)_i$.  

\subsubsection{Visualizing 2D Lidar Data Using OpenCV}  

Starting from this chapter, we will use real-world data collected from actual robots to verify whether our algorithms perform satisfactorily in real-world scenarios. This section and subsequent chapters will require some ROS bag files. Due to their large size, readers are advised to download the necessary datasets from the code repository associated with this book. If storage space is limited, you may choose to download only representative datasets for each chapter. The program in this section requires the data package located in the `2dmapping/` directory, while other chapters will use datasets from their respective directories.  

To facilitate testing different algorithms across various datasets, we have implemented an abstract interface for ROS bag processing. Readers only need to define callback functions for different message types. For example, in the demo code shown here, we need to read 2D scan messages from the bag file and pass them to a visualization program for rendering. In other chapters, these data may be fed into matching algorithms for registration. We leverage C++ lambda functions to achieve this flexible invocation:  

\begin{lstlisting}[language=c++,caption=src/ch6/test\_2dlidar\_io.cc]  
sad::RosbagIO rosbag_io(FLAGS_bag_path);  
rosbag_io  
.AddScan2DHandle("/pavo_scan_bottom",  
	[](Scan2d::Ptr scan) {  
		cv::Mat image;  
		sad::Visualize2DScan(scan, SE2(), image, Vec3b(255, 0, 0));  
		cv::imshow("scan", image);  
		cv::waitKey(20);  
		return true;  
	})  
.Go();  

void Visualize2DScan(Scan2d::Ptr scan, const SE2& pose, cv::Mat& image, const Vec3b& color, int image_size, float resolution, const SE2& pose_submap) {  
	if (image.data == nullptr) {  
		image = cv::Mat(image_size, image_size, CV_8UC3, cv::Vec3b(255, 255, 255));  
	}  
	
	for (size_t i = 0; i < scan->ranges.size(); ++i) {  
		if (scan->ranges[i] < scan->range_min || scan->ranges[i] > scan->range_max) {  
			continue;  
		}  
		
		double real_angle = scan->angle_min + i * scan->angle_increment;  
		double x = scan->ranges[i] * std::cos(real_angle);  
		double y = scan->ranges[i] * std::sin(real_angle);  
		
		if (real_angle < scan->angle_min + 30 * M_PI / 180.0 || real_angle > scan->angle_max - 30 * M_PI / 180.0) {  
			continue;  
		}  
		
		Vec2d psubmap = pose_submap.inverse() * (pose * Vec2d(x, y));  
		
		int image_x = int(psubmap[0] * resolution + image_size / 2);  
		int image_y = int(psubmap[1] * resolution + image_size / 2);  
		if (image_x >= 0 && image_x < image.cols && image_y >= 0 && image_y < image.rows) {  
			image.at<cv::Vec3b>(image_y, image_x) = cv::Vec3b(color[0], color[1], color[2]);  
		}  
	}  
	
	// Draw the robot's position  
	Vec2d pose_in_image =  
	pose_submap.inverse() * (pose.translation()) * double(resolution) + Vec2d(image_size / 2, image_size / 2);  
	cv::circle(image, cv::Point2f(pose_in_image[0], pose_in_image[1]), 5, cv::Scalar(color[0], color[1], color[2]), 2);  
}  
\end{lstlisting}  

As shown, this program uses the `sad::RosbagIO` class to read the `pavo_scan_bottom` messages from the bag file and passes them to a visualization function. The visualization function converts the lidar's range and angle measurements into Cartesian coordinates and renders them onto an image at a specified resolution. If the robot's pose is provided, the visualization can also display the scan data in motion. Here, we demonstrate the scan data in the robot's body frame by setting the input pose to the origin.

Now, please compile the `test_2dlidar_io` program and run the following command to view the laser scan data in the given bag file:

\begin{lstlisting}[language=sh, caption=Terminal command]
	/bin/test_2dlidar_io --bag_path ./dataset/sad/2dmapping/floor1.bag
\end{lstlisting}

Readers should see laser scan data similar to Figure~\ref{fig:2dscan}. Since the actual robot was moving, you should also observe structures from different locations in the scene. With good spatial imagination, one should be able to infer the robot's movement direction and surrounding environment.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/2d-lidar-mapping/2dscan}
	\caption{Example of single 2D scan data}
	\label{fig:2dscan}
\end{figure}

In scan data like Figure~\ref{fig:2dscan}, we call the actual laser hit points \textbf{end points}. End points have two physical meanings:
1. The end point itself represents an actual existing obstacle;
2. Along the line connecting the sensor to the end point, no other obstacles exist.

Note that the second meaning requires calculating the line from the sensor to the end point (the sensor doesn't measure this line - it only measures the end point, so we need to compute this line ourselves). If we want to calculate which grid cells this line passes through, it involves \textbf{ray casting algorithm} \cite{Ray1999} and \textbf{rasterization algorithm} \cite{Pineda1988}. We'll mention these again later in grid map construction and provide a simple implementation. However, in scan matching algorithms, we focus more on the first meaning and often ignore the second.

Under this premise, a single scan can be viewed as a simple 2D point set.

Now let us derive the mathematical model for scan matching algorithms. From the perspective of state estimation, 2D laser scan data can be denoted as observation data $\bm{z}$. It is obtained when the robot at pose $\bm{x}$ observes a certain map $\bm{m}$. Thus, the observation model can be simply expressed as:
\begin{equation}\label{key}
	\bm{z} = \bm{h} (\bm{x}, \bm{m}) + \bm{w},
\end{equation}
where $\bm{w}$ is the noise term. Our goal is to estimate $\bm{x}$ based on the observed $\bm{z}$ and $\bm{m}$. According to Bayesian estimation theory, $\bm{x}$ can be obtained through \textbf{Maximum a Posteriori} (MAP) or \textbf{Maximum Likelihood Estimation} (MLE):
\begin{equation}\label{key}
	\bm{x}_{\mathrm{MLE}} = \arg \max p(\bm{x}|\bm{z}, \bm{m}) = \arg \max p(\bm{z}|\bm{x}, \bm{m}).
\end{equation}

If we only consider the scan-to-scan problem, $\bm{m}$ can simply be written as the previous scan data. The key then becomes how to define the detailed form of the observation equation, i.e., how to compute the residual term for each observation. Here we present several typical solutions: point-to-point scan matching (ICP) \cite{Arun1987}, point-to-line scan matching (PL-ICP \cite{Censi2008} or ICL \cite{Alshawa2007}), and the Gaussian likelihood field method (or CSM \cite{Olson2009}). In 3D matching algorithms, we will further introduce other methods such as point-to-plane \cite{Park2003,Low2004} and NDT \cite{Biber2003,Magnusson2009,Rapp2015}. Since 2D scan matching does not involve surface elements, we will only discuss point-to-point and point-to-line algorithms here.

The specific definition of the observation equation involves several issues:
\begin{enumerate}
	\item How to select the points to be matched. In principle, all scanned points should participate in matching, but for efficiency considerations, \textbf{sampling} can be applied. There are many sampling methods, from uniform or random sampling to normal- or feature-based sampling, all of which can be used in practice.
	\item How to determine which specific map point corresponds to a scan point $(x,y)_i$. This is also known as the \textbf{data association} problem. This problem is typically solved using the nearest neighbor method introduced in the previous section, i.e., assuming that under the current estimated pose, the closest map point to the observed point is the matching point. In field-based methods, grid cells or fields in the map can also be used as matching points.
	\item After determining the scan point $(x,y)_i$ and its corresponding map point $\bm{m}_i$, how to compute the residual. This involves the modeling of residuals. The complete laser scan noise model (beam model) is complex with many parameters \cite{Cabaleiro2015}, and as a state estimation model, it is not smooth enough. In practice, we usually simplify it, and in the simplest case, we can directly model it as a 2D Gaussian distribution, i.e., $\bm{w} \sim \mathcal{N}(\bm{0}, \boldsymbol{\Sigma})$.
\end{enumerate}

As can be seen, a scan matching algorithm involves many choices at different stages, and there are numerous variants of basic methods in both industry and academia. We will introduce the origins of these variants starting from basic methods, but we will not attempt to cover all scan matching algorithms. To maintain consistency, we will use the same mathematical notation to describe the problems and provide code implementations for each algorithm.

First, let us look at the simplest point-to-point matching problem, also known as the Iterative Closest Point (ICP) algorithm \cite{Besl1992}. The ICP algorithm divides the scan matching problem into two steps: \textbf{data association} and \textbf{pose estimation}, and alternates between these two steps until convergence. In fact, regardless of how data association and pose estimation are specifically solved, as long as the algorithm involves alternating between these two steps, we can refer to it as an \textbf{ICP-like} algorithm \cite{Koide2020,segal2009generalized,Zhang2021a}. When the matching relationship is known, ICP can be solved in closed form, but this approach discards the possibility of further filtering outliers and makes point-to-point and point-to-plane methods appear different (note that from an optimization perspective, they are unified). For consistency, we will describe the problem in terms of residuals and optimization.

The pose of a 2D laser is described by translation and rotation angles\footnote{In the program, we use the SE2 interface, which is essentially the same as the SE3 interface. We can use the same notation for SE3 and SE2, such as matrix multiplication. In some literature, 2D poses are also referred to as \textbf{three-degree-of-freedom} poses.}, and can be simply written as:
\begin{equation}\label{key}
	\bm{x} = [x, y, \theta]^\top.
\end{equation}

Here, $\bm{x}$ describes a transformation from the robot's coordinate frame $B$ to the world frame $W$, denoted as $\bm{x} = \bm{T}_{WB}$ according to the convention of this book. Note that submap frames and their coordinate systems will be introduced later, so it is necessary to clarify the transformation relationship of $\bm{x}$. Suppose a scan point $\bm{p}_i^B$ in the robot's frame has distance and angle $r_i, \rho_i$. Then, based on the current laser pose, it can be transformed to the world frame:
\begin{equation}\label{key}
	\bm{p}^W_i = [x+r_i \cos (\rho_i + \theta), y+r_i \sin(\rho_i + \theta)]^\top.
\end{equation}
In 3D space, this can be written as:
\begin{equation}\label{key}
	\bm{p}^W_i = \bm{T}_{WB} \bm{p}^B_i.
\end{equation}

In the program, since the SE3 and SE2 interfaces are consistent, we do not deliberately distinguish between 3D and 2D poses in mathematical notation.

Assuming we find a nearest neighbor $\bm{q}_i^W$ near $\bm{p}_i^W$, we can easily construct the residual between $\bm{p}_i^W$ and $\bm{q}_i^W$:
\begin{equation}\label{key}
	\bm{e}_i = \bm{p}_i^W - \bm{q}_i^W,
\end{equation}

This residual describes the Euclidean geometric distance between two points. Clearly, this error uses world coordinates and is related to the robot's pose at that time. Thus, the robot pose estimation problem can be transformed into a least-squares problem with variables $x, y, \theta$:
\begin{equation}\label{key}
	(x,y,\theta)^* = \arg \min\limits_{\bm{x}} \sum_{i=1}^n \| \bm{e}_i \|_2^2.
\end{equation}
This least-squares problem can be solved by many existing solvers.

To solve the least-squares problem, we should provide the derivatives of $\bm{e}$ with respect to each state variable. The obvious advantage of 2D poses is that we no longer need to use manifold notation and can directly use the decomposed $x, y, \theta$\footnote{Of course, it is possible to unify them using manifold notation, but it is unnecessary.}. Based on the above definitions, we can easily obtain:
\begin{subequations}\label{key}
	\begin{align}
		\frac{\partial{\bm{e}_i}}{\partial x} &= [1, 0]^\top, \\
		\frac{\partial{\bm{e}_i}}{\partial y} &= [0, 1]^\top, \\
		\frac{\partial{\bm{e}_i}}{\partial \theta} &= [-r_i \sin (\rho_i + \theta), r_i \cos (\rho_i+\theta)]^\top.
	\end{align}
\end{subequations}

We can organize this into matrix form:
\begin{equation}\label{eq:dpw-dx}
	\frac{\partial \bm{e}_i}{\partial \bm{x}} = \begin{bmatrix}
		1 & 0\\
		0 & 1\\
		-r_i \sin (\rho_i + \theta) & r_i \cos (\rho_i+\theta)
	\end{bmatrix} \in \mathbb{R}^{3\times 2}.
\end{equation}

In the subsequent experimental section, we will use this Jacobian matrix to solve the Gauss-Newton method. It is important to note that if the state variables $x, y, \theta$ change, $\bm{q}_i$ will also change, altering the entire problem. If the state variables are initially set far from the optimal solution, $\bm{q}_i$ may be an incorrect point, making ICP-like methods highly dependent on the initial value of optimization. We will continue to discuss this issue later.

\subsection{Implementation of Point-to-Point ICP (Gauss-Newton)}  
Below we implement a 2D point-to-point ICP method by manually coding the Gauss-Newton approach. In each Gauss-Newton iteration, we recompute the nearest neighbors between points and then solve for the pose increment. The key points here are: (1) implementing nearest neighbor search, and (2) implementing Gauss-Newton iteration.  

Since the nearest neighbor data structure from the previous lecture used 3D points rather than 2D points, here we employ PCL's K-d tree for nearest neighbor search with 2D points. Thus, when setting the target point cloud, we need to build a K-d tree for it. Our 2D ICP class interface is as follows:  

\begin{lstlisting}[language=c++,caption=src/ch6/icp\_2d.h]  
	class Icp2d {  
		public:  
		using Point2d = pcl::PointXY;  
		using Cloud2d = pcl::PointCloud<Point2d>;  
		Icp2d() {}  
		
		/// Set the target scan  
		void SetTarget(Scan2d::Ptr target) {  
			target_scan_ = target;  
			BuildTargetKdTree();  
		}  
		
		/// Set the source scan to be aligned  
		void SetSource(Scan2d::Ptr source) { source_scan_ = source; }  
		
		/// Perform alignment using Gauss-Newton method  
		bool AlignGaussNewton(SE2& init_pose);  
		
		private:  
		// Build K-d tree for the target point cloud  
		void BuildTargetKdTree();  
		
		pcl::search::KdTree<Point2d> kdtree_;  
		Cloud2d::Ptr target_cloud_;  // Target cloud in PCL format  
		
		Scan2d::Ptr target_scan_ = nullptr;  
		Scan2d::Ptr source_scan_ = nullptr;  
	};  
\end{lstlisting}  

The `AlignGaussNewton` function implements 2D ICP based on Gauss-Newton iteration:  

\begin{lstlisting}[language=c++, caption=src/ch6/icp\_2d.cc]  
	bool Icp2d::AlignGaussNewton(SE2& init_pose) {  
		int iterations = 10;  
		double cost = 0, lastCost = 0;  
		SE2 current_pose = init_pose;  
		const float max_dis2 = 0.01;      // Maximum squared distance for nearest neighbors  
		const int min_effect_pts = 20;  // Minimum number of effective points  
		
		for (int iter = 0; iter < iterations; ++iter) {  
			Mat3d H = Mat3d::Zero();  
			Vec3d b = Vec3d::Zero();  
			cost = 0;  
			
			int effective_num = 0;  // Number of effective points  
			
			// Traverse source points  
			for (size_t i = 0; i < source_scan_->ranges.size(); ++i) {  
				float r = source_scan_->ranges[i];  
				if (r < source_scan_->range_min || r > source_scan_->range_max) {  
					continue;  
				}  
				
				float angle = source_scan_->angle_min + i * source_scan_->angle_increment;  
				float theta = current_pose.so2().log();  
				Vec2d pw = current_pose * Vec2d(r * std::cos(angle), r * std::sin(angle));  
				Point2d pt;  
				pt.x = pw.x();  
				pt.y = pw.y();  
				
				// Nearest neighbor search  
				std::vector<int> nn_idx;  
				std::vector<float> dis;  
				kdtree_.nearestKSearch(pt, 1, nn_idx, dis);  
				
				if (nn_idx.size() > 0 && dis[0] < max_dis2) {  
					effective_num++;  
					Mat32d J;  
					J << 1, 0, 0, 1, -r * std::sin(angle + theta), r * std::cos(angle + theta);  
					H += J * J.transpose();  
					
					Vec2d e(pt.x - target_cloud_->points[nn_idx[0]].x, pt.y -  
					target_cloud_->points[nn_idx[0]].y);  
					b += -J * e;  
					
					cost += e.dot(e);  
				}  
			}  
			
			if (effective_num < min_effect_pts) {  
				return false;  
			}  
			
			// Solve for dx  
			Vec3d dx = H.ldlt().solve(b);  
			if (isnan(dx[0])) {  
				break;  
			}  
			
			cost /= effective_num;  
			if (iter > 0 && cost >= lastCost) {  
				break;  
			}  
			
			LOG(INFO) << "iter " << iter << " cost = " << cost << ", effect num: " << effective_num;  
			
			current_pose.translation() += dx.head<2>();  
			current_pose.so2() = current_pose.so2() * SO2::exp(dx[2]);  
			lastCost = cost;  
		}  
		
		init_pose = current_pose;  
		LOG(INFO) << "estimated pose: " << current_pose.translation().transpose()  
		<< ", theta: " << current_pose.so2().log();  
		
		return true;  
	}  
\end{lstlisting}  

The Jacobian matrix here corresponds to the theoretical part introduced earlier. We limit the maximum squared distance for nearest neighbors (set to 0.01) and count the number of valid nearest neighbors, then compute their average error. Finally, the `current_pose` obtained from Gauss-Newton iteration is filled into the return result.

We also write a test program to evaluate the results of 2D ICP:

\begin{lstlisting}[language=c++,caption=src/ch6/test\_2d\_icp\_s2s.cc]
	rosbag_io.AddScan2DHandle("/pavo_scan_bottom",
	[&](Scan2d::Ptr scan) {
		current_scan = scan;
		
		if (last_scan == nullptr) {
			last_scan = current_scan;
			return true;
		}
		
		sad::Icp2d icp;
		icp.SetTarget(last_scan);
		icp.SetSource(current_scan);
		
		SE2 pose;
		if (FLAGS_method == "point2point") {
			icp.AlignGaussNewton(pose);
		} else if (fLS::FLAGS_method == "point2plane") {
			icp.AlignGaussNewtonPoint2Plane(pose);
		}
		
		cv::Mat image;
		sad::Visualize2DScan(last_scan, SE2(), image, Vec3b(255, 0, 0));    // target in blue
		sad::Visualize2DScan(current_scan, pose, image, Vec3b(0, 0, 255));  // source in red
		cv::imshow("scan", image);
		cv::waitKey(20);
		
		last_scan = current_scan;
		return true;
	})
	.Go();
\end{lstlisting}

This program registers the current scan data to the previous scan and visualizes the results using OpenCV. The previous frame is displayed in blue, while the current frame is shown in red. After registration, the two scans should align well with each other. Running this program allows real-time observation of the registration effect, as shown in Figure~\ref{fig:2dicp-s2s}. Readers can also monitor metrics such as the objective function value and the number of valid points for each ICP iteration in the terminal. However, due to the robot's motion, there will inevitably be some discrepancies between the two scans. Previously unexplored areas will appear in the current frame, and dynamic objects or motion distortion in the scan data itself may also interfere with the registration results. We can adjust the thresholds in ICP or parameters in the optimization model to mitigate the impact of dynamic objects to some extent.

This test program is also compatible with the point-to-plane ICP interface discussed later. Readers can use different gflags to test it.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.4\textwidth]{resources/2d-lidar-mapping/2dicp-s2s}
	\caption{Registration results of point-to-point ICP}
	\label{fig:2dicp-s2s}
\end{figure}

\subsection{Point-to-Line Scan Matching}  
In addition to point-to-point methods, ICP can also utilize other error formulations. The most common alternatives are point-to-line or point-to-plane approaches. Since 2D lidar data doesn't contain planes, we focus on the point-to-line formulation (which can be considered a lower-dimensional version of point-to-plane). The overall workflow of point-to-line ICP remains similar to point-to-point ICP, except that during nearest neighbor search, we need to find multiple neighbors (e.g., k neighbors), fit a line to these points, and then compute the perpendicular distance from the target point to this line. This method is called Point-to-line ICP (PL-ICP) \cite{Censi2008}.

Let these k nearest neighbors be $(\bm{x}_1, \ldots, \bm{x}_k), \forall i \in 1, \ldots k, \bm{x}_i \in \mathbb{R}^2$. In 3D space, the fitted line can be described by a direction vector $\bm{d}$ and origin $\bm{p}$, with fitting methods already introduced in Section~\ref{sec:line-fitting}. While lines in 3D space are more complex, in 2D space they can be simplified to a slope-intercept model. Let the line equation be:
\begin{equation}\label{key}
	a x + by + c = 0,
\end{equation}
where $a,b,c$ are line parameters. The line fitting can then be formulated as a least-squares parameter estimation problem:
\begin{equation}\label{key}
	(a,b,c)^* = \arg \min \sum_{i=1}^N \| a x_i + by_i + c \|_2^2 .
\end{equation}

We simply arrange the point coordinates into a matrix:
\begin{equation}\label{key}
	\bm{A} = \begin{bmatrix}
		x_1 & y_1 & 1 \\
		x_2 & y_2 & 1 \\
		& \ldots &\\
		x_k & y_k & 1
	\end{bmatrix},
\end{equation}
and find the minimum singular vector of $\bm{A}$.

After obtaining the line parameters $(a,b,c)$ from the nearest neighbors, the perpendicular distance from any point $(x,y)$ to this line can be expressed as:
\begin{equation}\label{key}
	d = \frac{ax+by+c}{\sqrt{a^2+b^2}}.
\end{equation}
Since the denominator is a constant that can be ignored, we can directly use the residual:
\begin{equation}\label{key}
	e = ax+by+c,
\end{equation}
as the objective function. The line equation also provides the corresponding Jacobian matrix:
\begin{equation}\label{key}
	\frac{\partial e}{\partial x} = a, \quad \frac{\partial e}{\partial y} = b.
\end{equation}
Thus, the fitted line results can guide the optimization direction. We will see similar results in 3D point-to-plane ICP later.

Now we incorporate the lidar's pose into the above discussion. Let the lidar's position and orientation be $\bm{x} = (x,y, \theta)$. For a lidar point with distance and angle $(r_i, \rho_i)$, we transform it to world coordinates to get $\bm{p}_i^w$. With line parameters $(a_i, b_i, c_i)$ fitted from its nearest neighbors, the Jacobian matrix of its residual $e_i$ with respect to the pose can be expressed using the chain rule:
\begin{equation}\label{key}
	\frac{\partial e_i}{\partial \bm{x}} = \frac{\partial e_i}{\partial \bm{p}_i^w} \frac{\partial \bm{p}_i^w}{\partial \bm{x}},
\end{equation} 
where the latter term is given in Equation \eqref{eq:dpw-dx}, and the former term is determined by the line parameters. Multiplying them together yields:
\begin{equation}\label{key}
	\frac{\partial e_i}{\partial \bm{x}} = [a_i, b_i, -a_i r_i \sin(\rho_i + \theta) + b_i r_i \cos(\rho_i + \theta)]^\top.
\end{equation}


\subsection{Implementation of Point-to-Line ICP (Gauss-Newton)}
Below we implement the algorithm described in the previous section. Its overall workflow is consistent with point-to-point ICP, and we only need to add an interface to the existing ICP class:

\begin{lstlisting}[language=c++,caption=src/ch6/icp\_2d.cc]
	bool Icp2d::AlignGaussNewtonPoint2Plane(SE2& init_pose) {
		int iterations = 10;
		double cost = 0, lastCost = 0;
		SE2 current_pose = init_pose;
		const float max_dis = 0.3;      // Maximum distance for nearest neighbors
		const int min_effect_pts = 20;  // Minimum number of effective points
		
		for (int iter = 0; iter < iterations; ++iter) {
			Mat3d H = Mat3d::Zero();
			Vec3d b = Vec3d::Zero();
			cost = 0;
			
			int effective_num = 0;  // Number of effective points
			
			// Traverse source points
			for (size_t i = 0; i < source_scan_->ranges.size(); ++i) {
				float r = source_scan_->ranges[i];
				if (r < source_scan_->range_min || r > source_scan_->range_max) {
					continue;
				}
				
				float angle = source_scan_->angle_min + i * source_scan_->angle_increment;
				float theta = current_pose.so2().log();
				Vec2d pw = current_pose * Vec2d(r * std::cos(angle), r * std::sin(angle));
				Point2d pt;
				pt.x = pw.x();
				pt.y = pw.y();
				
				// Find 5 nearest neighbors
				std::vector<int> nn_idx;
				std::vector<float> dis;
				kdtree_.nearestKSearch(pt, 5, nn_idx, dis);
				
				std::vector<Vec2d> effective_pts;  // Effective points
				for (int j = 0; j < nn_idx.size(); ++j) {
					if (dis[j] < max_dis) {
						effective_pts.emplace_back(
						Vec2d(target_cloud_->points[nn_idx[j]].x, target_cloud_->points[nn_idx[j]].y));
					}
				}
				
				if (effective_pts.size() < 3) {
					continue;
				}
				
				// Fit line and assemble J, H and error
				Vec3d line_coeffs;
				if (math::FitLine2D(effective_pts, line_coeffs)) {
					effective_num++;
					Vec3d J;
					J << line_coeffs[0], line_coeffs[1],
					-line_coeffs[0] * r * std::sin(angle + theta) + line_coeffs[1] * r * std::cos(angle + theta);
					H += J * J.transpose();
					
					double e = line_coeffs[0] * pw[0] + line_coeffs[1] * pw[1] + line_coeffs[2];
					b += -J * e;
					
					cost += e * e;
				}
			}
			
			if (effective_num < min_effect_pts) {
				return false;
			}
			
			// solve for dx
			Vec3d dx = H.ldlt().solve(b);
			if (isnan(dx[0])) {
				break;
			}
			
			cost /= effective_num;
			if (iter > 0 && cost >= lastCost) {
				break;
			}
			
			LOG(INFO) << "iter " << iter << " cost = " << cost << ", effect num: " << effective_num;
			
			current_pose.translation() += dx.head<2>();        current_pose.so2() = current_pose.so2() * 
			SO2::exp(dx[2]);
			lastCost = cost;
		}
		
		init_pose = current_pose;
		LOG(INFO) << "estimated pose: " << current_pose.translation().transpose()
		<< ", theta: " << current_pose.so2().log();
		
		return true;
	}
\end{lstlisting}

In the implementation, we search for five nearest neighbors around the target point and use them to fit a local line segment. The 2D line fitting algorithm is provided in common/math\_utils.h:

\begin{lstlisting}[language=c++,caption=src/common/math\_utils.h]
	template <typename S>
	bool FitLine2D(const std::vector<Eigen::Matrix<S, 2, 1>>& data, Eigen::Matrix<S, 3, 1>& coeffs) {
		if (data.size() < 2) {
			return false;
		}
		
		Eigen::MatrixXd A(data.size(), 3);
		for (int i = 0; i < data.size(); ++i) {
			A.row(i).head<2>() = data[i].transpose();
			A.row(i)[2] = 1.0;
		}
		
		Eigen::JacobiSVD svd(A, Eigen::ComputeThinV);
		coeffs = svd.matrixV().col(2);
		return true;
	}
\end{lstlisting}

Note its similarity to the 3D plane fitting algorithm. Finally, the test case from the previous section can be used to examine the registration effect of point-to-line ICP. Since its results are similar to point-to-point ICP, we won't include additional figures here - readers are encouraged to experiment themselves (using the test program from the previous section with --method=point2plane). Generally speaking, point-to-line ICP performs better than point-to-point ICP, though at the cost of greater computational requirements due to the need to compute multiple nearest neighbors.

\subsection{Likelihood Field Method}
\label{sec:likelihood-field}

Point-to-point or point-to-line ICP can be used for both scan-to-scan matching and scan-to-map registration. If we store the map as discrete 2D points, ICP-like methods can be applied to map matching in the same way. However, in 2D SLAM, we typically store the map as an \textbf{occupancy grid map} with a certain resolution. This image-like map has an update mechanism that provides some filtering effect against dynamic objects (which we will implement in the next section). Thus, we can design a registration method that aligns scan data with grid maps in an ICP-like manner. The likelihood field method (also known as Gaussian Likelihood Field) introduced in this section is precisely such an approach for registering scan data with grid maps \cite{Thrun2005}.

In point-to-point ICP, we compute Euclidean distance errors between target points and their nearest neighbors in another point cloud. These errors grow with the squared distance between points and ultimately form the objective function through summation. Intuitively, we can imagine a \textbf{spring} installed between each point and its nearest neighbor. The collective pull of these springs eventually brings the point cloud to the position of minimum energy. However, in ICP methods, we must reinstall these springs during each iteration, which is computationally expensive. 

An alternative approach is to consider that the point cloud generates a \textbf{field} in space rather than installing springs between points. This field attracts nearby point clouds, with its attractive force decaying quadratically with distance. This is essentially the idea behind the likelihood field method. We can define a decaying field around each point in the map. Unlike physical fields, however, the fields in computer programs have defined \textbf{effective ranges} and \textbf{resolutions}. The field can decay quadratically or follow a Gaussian distribution with distance. When a measured point falls near the field, we can use the field's value as the error function for that point.

The likelihood field method can be used to register either two scan datasets or a scan dataset with a map dataset. More commonly, it works in conjunction with grid maps for map matching. To perform registration, we first need to generate this likelihood field. In this section, we generate the likelihood field only for point cloud data. Later, after introducing occupancy grid maps, we can also generate likelihood field maps for grid maps. The likelihood field can be further bound with submaps to achieve simple and fast registration. Here, we "draw" a distance-decaying circle around each point. These circles are fixed and can be precomputed.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/2d-lidar-mapping/likelihood-field}
	\caption{An example of a likelihood field}
	\label{fig:likelihood-field}
\end{figure}

Figure~\ref{fig:likelihood-field} shows a 2D scan dataset and its corresponding likelihood field. Visually, we can observe that the likelihood field radiates from each scan point and gradually decays with distance. We can customize its range and decay characteristics. The likelihood field essentially describes a distance function between each pixel and its nearest scan point, also referred to as a distance transform map in some applications \cite{Felzenszwalb2012}. With the likelihood field, we no longer need nearest neighbor structures like K-d trees to find the closest point for a given point; instead, we can directly use the field's readings.

Next, we derive the scan matching algorithm based on the likelihood field. The readings from the likelihood field can directly serve as the objective function for registration. Consider a point $\bm{p}^B_i$ transformed by pose $\bm{x}$ to obtain $\bm{p}^W_i$ in the world coordinate frame. Simultaneously, there exists a likelihood field $\pi$ in the world coordinate frame\footnote{Note that the likelihood field doesn't necessarily need to be maintained in the world coordinate frame; later discussions will show it is primarily maintained in submap coordinates.}. The reading of this point in the likelihood field $\pi$ is $\pi(\bm{p}^W_i)$. Thus, $\bm{x}$ can be obtained by solving the optimization problem:
\begin{equation}\label{key}
	\bm{x}^* = \arg \min_{\bm{x}} \sum_{i=1}^{n} \| \pi(\bm{p}_i^W) \|_2^2.
\end{equation}

The Jacobian matrix of the $\pi$ function with respect to pose $\bm{x}$ can be decomposed via the chain rule:
\begin{equation}\label{key}
	\frac{\partial \pi}{\partial \bm{x}} = \frac{\partial \pi}{\partial \bm{p}^W_i} \frac{\partial \bm{p}^W_i}{\partial \bm{x}}.
\end{equation}
The latter term is given in Equation \eqref{eq:dpw-dx}, so we focus on the former term.

Since the likelihood field is stored as an image, $\bm{p}^W_i$ must be sampled at a certain resolution. Let the transformation from $\bm{p}^W_i$ to its image coordinates $\bm{p}^f_i$ be:
\begin{equation}\label{key}
	\bm{p}^f_i = \alpha \bm{p}^W_i + \bm{c},
\end{equation}
where $\alpha$ is the scaling factor and $\bm{c} \in \mathbb{R}^2$ is the offset of the image center. Note that image coordinates typically start from the top-left corner, while object coordinates usually originate from the center, so the offset is typically half the image dimensions. The derivative of function $\pi$ with respect to $\bm{p}^W_i$ is:
\begin{equation}\label{key}
	\frac{\partial \pi}{\partial \bm{p}^W_i} = \frac{\partial \pi}{\partial \bm{p}^f_i} \frac{\partial \bm{p}^f_i}{\partial \bm{p}^W_i}= \alpha [\Delta \pi_x, \Delta \pi_y]^\top,
\end{equation}
where $[\Delta \pi_x, \Delta \pi_y]$ represents the gradient of the likelihood field in the image. Since we define the likelihood function for each point as a smooth function, its gradient is equally reliable. Multiplying these two matrices yields the Jacobian matrix of each residual with respect to the pose:
\begin{equation}\label{key}
	\frac{\partial \pi}{\partial \bm{x}} = [\alpha \Delta \pi_x, \alpha \Delta \pi_y, -\alpha \Delta \pi_x r_i \sin(\rho_i+\theta) + \alpha \Delta \pi_y r_i \cos(\rho_i +\theta)]^\top.
\end{equation}

Using this Jacobian matrix, we can implement registration based on the Gauss-Newton method.

\subsection{Implementation of Likelihood Field Method (Gauss-Newton)}  

When implementing the likelihood field method, we need to generate the corresponding likelihood field when setting the target point cloud. Each point in the likelihood field can use a pre-generated, fixed-size template, which is then "pasted" onto each point of the target point cloud.  

\begin{lstlisting}[language=c++,caption=src/ch6/likelihood\_field.cc]  
	class LikelihoodField {  
		public:  
		/// 2D field template, generated when setting target scan or map  
		struct ModelPoint {  
			ModelPoint(int dx, int dy, float res) : dx_(dx), dy_(dy), residual_(res) {}  
			int dx_ = 0;  
			int dy_ = 0;  
			float residual_ = 0;  
		};  
		
		private:  
		std::vector<ModelPoint> model_;  // 2D template  
	};  
	
	void LikelihoodField::BuildModel() {  
		const int range = 20;  // Template size in pixels  
		for (int x = -range; x <= range; ++x) {  
			for (int y = -range; y <= range; ++y) {  
				model_.emplace_back(x, y, std::sqrt((x * x) + (y * y)));  
			}  
		}  
	}  
	
	void LikelihoodField::SetTargetScan(Scan2d::Ptr scan) {  
		target_ = scan;  
		
		// Generate field function on target points  
		field_ = cv::Mat(1000, 1000, CV_32F, 30.0);  
		
		for (size_t i = 0; i < scan->ranges.size(); ++i) {  
			if (scan->ranges[i] < scan->range_min || scan->ranges[i] > scan->range_max) {  
				continue;  
			}  
			
			double real_angle = scan->angle_min + i * scan->angle_increment;  
			double x = scan->ranges[i] * std::cos(real_angle) * resolution_ + 500;  
			double y = scan->ranges[i] * std::sin(real_angle) * resolution_ + 500;  
			
			// Fill field function around (x,y)  
			for (auto& model_pt : model_) {  
				int xx = int(x + model_pt.dx_);  
				int yy = int(y + model_pt.dy_);  
				if (xx >= 0 && xx < field_.cols && yy >= 0 && yy < field_.rows &&  
				field_.at<float>(yy, xx) > model_pt.residual_) {  
					field_.at<float>(yy, xx) = model_pt.residual_;  
				}  
			}  
		}  
	}  
\end{lstlisting}  

We use a 1000$\times$1000 pixel image to store the likelihood field data. This class generates a template with a 20-pixel edge length during construction and then applies this template to each point.  

Once the likelihood field is generated, we can use the previously described Gauss-Newton iteration method to register two scan datasets.  

\begin{lstlisting}[language=c++,caption=src/ch6/likelihood\_field.cc]  
	bool LikelihoodField::AlignGaussNewton(SE2& init_pose) {  
		int iterations = 10;  
		double cost = 0, lastCost = 0;  
		SE2 current_pose = init_pose;  
		const int min_effect_pts = 20;  // Minimum number of effective points  
		const int image_boarder = 20;   // Image border margin  
		
		for (int iter = 0; iter < iterations; ++iter) {  
			Mat3d H = Mat3d::Zero();  
			Vec3d b = Vec3d::Zero();  
			cost = 0;  
			
			int effective_num = 0;  // Number of effective points  
			
			// Traverse source points  
			for (size_t i = 0; i < source_->ranges.size(); ++i) {  
				float r = source_->ranges[i];  
				if (r < source_->range_min || r > source_->range_max) {  
					continue;  
				}  
				
				float angle = source_->angle_min + i * source_->angle_increment;  
				float theta = current_pose.so2().log();  
				Vec2d pw = current_pose * Vec2d(r * std::cos(angle), r * std::sin(angle));  
				
				// Image coordinates in the field  
				Vec2i pf = (pw * resolution_ + Vec2d(500, 500)).cast<int>();  
				
				if (pf[0] >= image_boarder && pf[0] < field_.cols - image_boarder && pf[1] >= image_boarder &&  
				pf[1] < field_.rows - image_boarder) {  
					effective_num++;  
					
					// Image gradient  
					float dx = 0.5 * (field_.at<float>(pf[1], pf[0] + 1) - field_.at<float>(pf[1], pf[0] - 1));  
					float dy = 0.5 * (field_.at<float>(pf[1] + 1, pf[0]) - field_.at<float>(pf[1] - 1, pf[0]));  
					
					Vec3d J;  
					J << resolution_ * dx, resolution_ * dy,  
					-resolution_ * dx * r * std::sin(angle + theta) + resolution_ * dy * r * std::cos(angle + theta);  
					H += J * J.transpose();  
					
					float e = field_.at<float>(pf[1], pf[0]);  
					b += -J * e;  
					
					cost += e * e;  
				}  
			}  
			
			if (effective_num < min_effect_pts) {  
				return false;  
			}  
			
			// solve for dx  
			Vec3d dx = H.ldlt().solve(b);  
			if (isnan(dx[0])) {  
				break;  
			}  
			
			cost /= effective_num;  
			if (iter > 0 && cost >= lastCost) {  
				break;  
			}  
			
			LOG(INFO) << "iter " << iter << " cost = " << cost << ", effect num: " << effective_num;  
			
			current_pose.translation() += dx.head<2>();  
			current_pose.so2() = current_pose.so2() * SO2::exp(dx[2]);  
			lastCost = cost;  
		}  
		
		init_pose = current_pose;  
		return true;  
	}  
\end{lstlisting}  

We only need to replace the residuals and Jacobians from the previous ICP with those from the likelihood field. Readers can run the provided test program to view the matching results of the 2D likelihood field method:  

\begin{lstlisting}[language=sh,caption=Terminal input:]  
	bin/test_2d_icp_likelihood   
\end{lstlisting}  

In addition to displaying the registration results, this program also shows real-time single-frame likelihood field data. Readers should observe that the likelihood field aligns with the scan data, as shown in Figure~\ref{fig:scan-and-field}.  

\begin{figure}[!htp]  
	\centering  
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/ch6-scan-and-filed.png}  
	\caption{Real-time scan data and likelihood field data}  
	\label{fig:scan-and-field}  
\end{figure}

\subsection{Implementation of Likelihood Field Method (g2o)}
Below we demonstrate how to use the g2o optimizer \cite{Kummerle2011} to implement a likelihood field-based scan matching algorithm. By using an optimizer, we can more conveniently employ different iteration strategies and set robust kernel functions to achieve more robust matching algorithms. In fact, all the registration methods implemented earlier can be adapted to an optimizer-based approach. Now we define the $\mathrm{SE}(2)$ pose vertex and the observation error edges corresponding to each scan point.

\begin{lstlisting}[language=c++,caption=src/ch6/g2o\_types.h]
	class VertexSE2 : public g2o::BaseVertex<3, SE2> {
		public:
		EIGEN_MAKE_ALIGNED_OPERATOR_NEW
		
		void setToOriginImpl() override { _estimate = SE2(); }
		void oplusImpl(const double* update) override {
			_estimate.translation()[0] += update[0];
			_estimate.translation()[1] += update[1];
			_estimate.so2() = _estimate.so2() * SO2::exp(update[2]);
		}
	};
	
	class EdgeSE2LikelihoodFiled : public g2o::BaseUnaryEdge<1, double, VertexSE2> {
		public:
		EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
		EdgeSE2LikelihoodFiled(const cv::Mat& field_image, double range, double angle, float resolution 
		= 10.0) : field_image_(field_image), range_(range), angle_(angle), resolution_(resolution) {}
		
		void computeError() override {
			VertexSE2* v = (VertexSE2*)_vertices[0];
			SE2 pose = v->estimate();
			Vec2d pw = pose * Vec2d(range_ * std::cos(angle_), range_ * std::sin(angle_));
			Vec2i pf = (pw * resolution_ + Vec2d(field_image_.rows / 2, field_image_.cols / 2)).cast<int>(); 
			
			if (pf[0] >= image_boarder_ && pf[0] < field_image_.cols - image_boarder_ && pf[1] >= 
			image_boarder_ && pf[1] < field_image_.rows - image_boarder_) {
				_error[0] = field_image_.at<float>(pf[1], pf[0]);
			} else {
				_error[0] = 0;
				setLevel(1);
			}
		}
		
		void linearizeOplus() override {
			VertexSE2* v = (VertexSE2*)_vertices[0];
			SE2 pose = v->estimate();
			float theta = pose.so2().log();
			Vec2d pw = pose * Vec2d(range_ * std::cos(angle_), range_ * std::sin(angle_));
			Vec2i pf = (pw * resolution_ + Vec2d(field_image_.rows / 2, field_image_.cols / 2)).cast<int>();
			
			if (pf[0] >= image_boarder_ && pf[0] < field_image_.cols - image_boarder_ && pf[1] >= 
			image_boarder_ && pf[1] < field_image_.rows - image_boarder_) {
				// Image gradient
				float dx = 0.5 * (field_image_.at<float>(pf[1], pf[0] + 1) - field_image_.at<float>(pf[1], pf[0] - 1));
				float dy = 0.5 * (field_image_.at<float>(pf[1] + 1, pf[0]) - field_image_.at<float>(pf[1] - 1, pf[0]));
				
				_jacobianOplusXi << resolution_ * dx, resolution_ * dy,
				-resolution_ * dx * range_ * std::sin(angle_ + theta) +
				resolution_ * dy * range_ * std::cos(angle_ + theta);
			} else {
				_jacobianOplusXi.setZero();
				setLevel(1);
			}
		}
		
		private:
		const cv::Mat& field_image_;
		double range_;
		double angle_;
		float resolution_ = 10.0;
		inline static const int image_boarder_ = 10;
	};
\end{lstlisting}

The Jacobian matrix here is consistent with our previous derivation, except that the likelihood field image has been moved inside the class for quick lookup of corresponding field function values and their gradients. Next, we only need to convert the iteration process from the Gauss-Newton method into an optimization problem.

\begin{lstlisting}[language=c++,caption=src/ch6/likelihood\_field.cc]
	bool LikelihoodField::AlignG2O(SE2& init_pose) {
		using BlockSolverType = g2o::BlockSolver<g2o::BlockSolverTraits<3, 1>>;
		using LinearSolverType = g2o::LinearSolverCholmod<BlockSolverType::PoseMatrixType>;
		auto* solver = new g2o::OptimizationAlgorithmLevenberg(
		g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
		g2o::SparseOptimizer optimizer;
		optimizer.setAlgorithm(solver);
		
		auto* v = new VertexSE2();
		v->setId(0);
		v->setEstimate(init_pose);
		optimizer.addVertex(v);
		
		// Traverse source points
		for (size_t i = 0; i < source_->ranges.size(); ++i) {
			float r = source_->ranges[i];
			if (r < source_->range_min || r > source_->range_max) {
				continue;
			}
			
			float angle = source_->angle_min + i * source_->angle_increment;
			auto e = new EdgeSE2LikelihoodFiled(field_, r, angle, resolution_);
			e->setVertex(0, v);
			e->setInformation(Eigen::Matrix<double, 1, 1>::Identity());
			optimizer.addEdge(e);
		}
		
		optimizer.setVerbose(true);
		optimizer.initializeOptimization();
		optimizer.optimize(10);
		
		init_pose = v->estimate();
		return true;
	}
\end{lstlisting}

This implements the g2o-based 2D scan matching algorithm. By adding --method=g2o to the test program in Section~\ref{sec:likelihood-field}, you can test the optimizer version of likelihood field matching. Since the results are similar, we won't include result images in this section. Readers can also implement versions based on Ceres or other optimizers following similar principles. Additionally, we can perform linear interpolation on the likelihood field image (Figure~\ref{fig:scan-and-field}) to obtain more accurate error functions. We leave these two aspects as exercises.

\section{Occupancy Grid Map}  
\subsection{Principle of Occupancy Grid Map}  
After performing scan matching, we obtain the relative motion between two sets of scan data. This process is equivalent to the **localization** problem in SLAM. Now, let us examine the **mapping** component.  

If we use the scan-to-map approach for scan matching, we can derive its pose relative to the map. Naturally, we can merge this scan data into the map to form a local map. However, there are some considerations in the actual map-building process. For example, should the map be constructed all at once or piece by piece? Should all scan points be combined into a single map, or should strategies like overlapping and refreshing be implemented? Early 2D SLAM solutions often adopted a simpler, monolithic map-building approach, but this method has many limitations. Therefore, this book introduces a more flexible management model based on **grid maps** and **submaps**.  

First, let us discuss the **occupancy grid map**. An occupancy grid is a map format that stores occupancy probabilities in a grid-like (or image-like) structure. A grid is a very simple form of a 2D map. It divides the map into many small planar cells, each storing custom information. The organization of this information is highly flexible. If obstacle information is stored, it is called an obstacle grid map, which can be used for path planning \cite{Tsardoulias2016}. In some applications, semantic information is also stored in grids, known as semantic grids \cite{Qi2020}. Grid maps are often associated with images, where each grid cell corresponds to a pixel. The storage and visualization of grid maps can be implemented using image libraries like OpenCV. Overall, grid maps are a widely used method for representing dense information on a 2D plane.  

In many robotic applications, people are only concerned with the **traversability** of each grid cell and not more complex semantic information (though passenger vehicles are an exception, which is why grid maps are rarely used in outdoor vehicles). Therefore, we only need to indicate whether a grid cell is occupied by an obstacle. The presence of an obstacle is probabilistic. Before scanning the map, we have no knowledge of whether obstacles exist, so the occupancy probability should initially be set to 0.5. If a grid cell is observed multiple times to contain an obstacle, its occupancy probability gradually increases to 1. Conversely, if a grid cell is repeatedly observed to be traversable, its probability decreases to 0. The endpoint measured by a 2D laser sensor indicates that the grid is occupied, while the line connecting the sensor to the endpoint indicates that the grid is traversable. Thus, 2D laser scan data can be easily converted into a grid format.  

It is worth noting that some applications use **occupancy** grids to represent obstacles, while others use **traversability** probability to indicate whether a grid is passable. These two probabilities are inversely related but functionally equivalent. In an occupancy grid, a probability of 1 means the grid contains an obstacle, whereas in a traversability grid, a probability of 1 means the grid is obstacle-free. In practice, both approaches can be used freely.  

On the other hand, grid maps can be dynamically updated, and not every grid cell's probability converges to 0 or 1. If a robot observes a grid cell as an obstacle multiple times, its occupancy probability will continuously rise. If a cell is repeatedly observed as empty, its occupancy probability should decrease. If an obstacle initially exists but later moves away, the grid cell's probability will first increase and then decrease. In summary, an occupancy grid map should satisfy the following descriptions:  

\begin{enumerate}  
	\item It stores the probability of each grid cell being occupied by an obstacle in a grid-like format. This probability should be a floating-point number between 0 and 1.  
	\item The grid has a certain resolution and is typically dense.  
	\item The occupancy probability changes with observations. Mathematically, the update logic for occupancy probability should comply with probabilistic principles. However, from an engineering perspective, simpler metrics like observation counts can also be used.  
\end{enumerate}  

\begin{figure}[!htp]  
	\centering  
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/grid-and-casting.pdf}  
	\caption{Occupancy grid and ray casting process for 2D LiDAR}  
	\label{fig:grid-and-casting}  
\end{figure}  

Figure~\ref{fig:grid-and-casting}~shows an example of a 2D LiDAR scan being converted into grid cells. When a laser beam is emitted from the sensor, the endpoint corresponds to an occupied grid cell, while the path from the sensor center to the endpoint is considered traversable. Note that this geometric model is only valid for **2D** robots. If the robot has significant height or the laser beams are tilted, this model no longer applies. In such cases, the 2D LiDAR occupancy grid can only indicate whether obstacles exist **at this specific height**. Obstacles at other heights, such as low steps, tables, or hanging objects, may still render the robot unable to pass. Therefore, if the robot's motion cannot be simplified to 2D, the map must account for the influence of obstacles at other heights.  

Due to the resolution limitations of grid maps, converting continuous laser beams into probability updates for each grid cell involves a **rasterization** process, as illustrated on the right side of Figure~\ref{fig:grid-and-casting}~. Rasterization is a concept in computer graphics that describes the conversion of geometric shapes into grid-based outputs. In 2D grid maps, we can choose to compute the rasterization result for each laser beam. However, if the LiDAR's angular resolution is high or the measurement range is long, this process can be time-consuming. Alternatively, we can precompute a fixed-size template region. The former requires rasterizing each laser scan line, while the latter involves rasterizing each template point. We will implement both algorithms and compare their performance.

\subsection{Map Generation Based on Bresenham's Algorithm}  
Bresenham's algorithm is a rasterization method for straight lines, commonly used for vectorization of geometric lines \cite{Cohen-Or1997}. It can be implemented entirely with integer operations, making it highly efficient. Since grid map coordinates are inherently integer-based, Bresenham's algorithm is well-suited for updating grid maps.  

Let the robot's origin in the map coordinate system be $\bm{p}_1$, and an endpoint be $\bm{p}_2$, both with integer coordinates. We aim to fill a straight line from $\bm{p}_1$ to $\bm{p}_2$ in the map, marking these cells as traversable. The Bresenham algorithm proceeds as follows:  

\begin{enumerate}  
	\item Compute $[dx, dy] = p2 - p1$, representing the direction of coordinate growth.  
	\item Compare $|dx|$ and $|dy|$, selecting the larger one as the primary growth axis (assume the $x$-axis for now).  
	\item Initialize $(x, y)$ at $\bm{p}_1$. Since the line's slope is $dy/dx$, each time $x$ increments by 1, the error between the discrete point and the true line increases by $dy/dx$. When this error exceeds 0.5, increment $y$ by 1 and decrease the error by 1.  
	\item Repeat until $(x, y)$ reaches $\bm{p}_2$.  
\end{enumerate}  

Step 3 involves floating-point operations, but we prefer integer-only computation. Thus, we multiply all terms and conditions in Step 3 by $2dx$ and subtract $dx$, transforming it into:  

\begin{enumerate}  
	\item Compute $[dx, dy] = p2 - p1$, representing the direction of coordinate growth.  
	\item Compare $|dx|$ and $|dy|$, selecting the larger one as the primary growth axis (assume the $x$-axis for now).  
	\item Initialize $(x, y)$ at $\bm{p}_1$ and set the initial error $e = -dx$. Each time $x$ increments by 1, $e$ increases by $2dy$. If $e > 0$, increment $y$ by 1 and subtract $2dx$ from $e$.  
	\item Repeat until $(x, y)$ reaches $\bm{p}_2$.  
\end{enumerate}  

This avoids floating-point and division operations, using only additions and multiplications. Similarly, if the $y$-axis is the primary growth direction, swap the roles of $x$ and $y$.  

The implementation of Bresenham's algorithm in the grid map is as follows:  

\begin{lstlisting}[language=c++,caption=src/ch6/occupancy\_map.cc]  
void OccupancyMap::BresenhamFilling(const Vec2i& p1, const Vec2i& p2) {  
	int dx = p2.x() - p1.x();  
	int dy = p2.y() - p1.y();  
	int ux = dx > 0 ? 1 : -1;  
	int uy = dy > 0 ? 1 : -1;  
	
	dx = abs(dx);  
	dy = abs(dy);  
	int x = p1.x();  
	int y = p1.y();  
	
	if (dx > dy) {  
		// Increment along x  
		int e = -dx;  
		for (int i = 0; i < dx; ++i) {  
			x += ux;  
			e += 2 * dy;  
			if (e >= 0) {  
				y += uy;  
				e -= 2 * dx;  
			}  
			
			if (Vec2i(x, y) != p2) {  
				SetPoint(Vec2i(x, y), false);  
			}  
		}  
	} else {  
		int e = -dy;  
		for (int i = 0; i < dy; ++i) {  
			y += uy;  
			e += 2 * dx;  
			if (e >= 0) {  
				x += ux;  
				e -= 2 * dy;  
			}  
			if (Vec2i(x, y) != p2) {  
				SetPoint(Vec2i(x, y), false);  
			}  
		}  
	}  
}  

/// Set whether a grid cell is occupied  
void OccupancyMap::SetPoint(const Vec2i& pt, bool occupy) {  
	int x = pt[0], y = pt[1];  
	if (x < 0 || y < 0 || x >= occupancy_grid_.cols || y >= occupancy_grid_.rows) {  
		if (occupy) {  
			has_outside_pts_ = true;  
		}  
		return;  
	}  
	
	/// Apply upper and lower bounds  
	uchar value = occupancy_grid_.at<uchar>(y, x);  
	if (occupy) {  
		if (value > 117) {  
			occupancy_grid_.ptr<uchar>(y)[x] -= 1;  
		}  
	} else {  
		if (value < 137) {  
			occupancy_grid_.ptr<uchar>(y)[x] += 1;  
		}  
	}  
}  
\end{lstlisting}

Now, readers are invited to compile and run the `test_occupancy_grid` program:
\begin{lstlisting}[language=sh,caption=Terminal input:]
	bin/test_occupancy_grid --bag_path ./dataset/sad/2dmapping/floor1.bag 
\end{lstlisting}

You can specify different filling methods using the `--method` option. The program will display the grid map results generated from a single scan, as shown in Figure~\ref{fig:oc-grid-one-scan}~. The final grid map is the superposition of these individual grid maps. As can be observed, the obstacles and traversable areas obtained from 2D LiDAR scans match our intuitive expectations.

The LiDAR used in our experiments does not have 360-degree coverage, leaving a blind spot behind the robot. For most robots with substantial height, it's impossible to completely suspend sensors in midair, inevitably causing partial occlusion of sensor data.

In this chapter's test program, we can observe that the template-based algorithm requires approximately 10-20ms to complete template filling, while the line-based method takes less than 1 millisecond - a significant difference. This is because the template method processes significantly more points than the line method (hundreds of thousands versus hundreds). If the LiDAR's angular resolution increases or the detection range extends, the template method can limit its computation range to save processing time.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.4\textwidth]{resources/2d-lidar-mapping/oc-grid-one-scan.png}
	\caption{Grid map generated from a single scan}
	\label{fig:oc-grid-one-scan}
\end{figure}

In template-based grid map updating, the sizes of submaps and templates should be set according to the actual sensor range. If the template is too small, distant scan data may not be fully utilized; if submap size is insufficient, frequent submap expansion may occur, introducing unnecessary computations. Here we've used parameters suitable for a 20m LiDAR range, with both grid template and submap sizes configured accordingly. For readers using longer-range LiDARs, these parameters should be appropriately increased. Note that algorithm efficiency will change noticeably with parameter adjustments - readers are encouraged to experiment with different settings.

\section{Submaps}  
\subsection{Principle of Submaps}  
Next, we integrate the matching algorithm with grid maps, utilizing the grid map update mechanism to combine matched data.  

Furthermore, we can group several matched results together to form \textbf{submaps}. Submaps serve as an intermediate data organization level between individual scan data and the global map. They flexibly aggregate multiple scans in chronological order and can be used in both 2D LiDAR SLAM and 3D LiDAR SLAM systems.  

Each submap is assigned an adjustable pose denoted by $\bm{T}_{WS} \in \mathrm{SE}(2)$, where $W$ represents the world coordinate system and $S$ represents the submap coordinate system. During scan matching, the scan-to-map algorithm essentially computes the pose relationship $\bm{T}_{SC}$ between the current LiDAR scan and the submap, where $C$ denotes the current scan coordinate system. Thus, the world coordinate of each scan can be expressed as:  
\begin{equation}\label{key}  
\bm{T}_{WC} = \bm{T}_{WS} \bm{T}_{SC}.  
\end{equation}  

This approach decouples the \textbf{submap pose} variable $\bm{T}_{WS}$. During scan matching, we compute $\bm{T}_{SC}$ for the current LiDAR scan, which remains fixed once determined. When adjusting the overall map shape, we only need to modify each submap's $\bm{T}_{WS}$ without altering internal submap contents (i.e., per-scan $\bm{T}_{SC}$). Consequently, loop closure detection can treat submaps as fundamental units without considering individual keyframe poses in the world coordinate system.  

We implement a submap-based 2D LiDAR SLAM system following this logic:  
\begin{enumerate}  
	\item Each submap is associated with a likelihood field and a grid map.  
	\item The current scan is always matched against the current submap to determine its pose within the submap\footnote{This step is flexible in implementation. With sufficient computational resources, matching against historical submaps is also feasible.}.  
	\item Keyframes are selected based on travel distance or rotation thresholds.  
	\item If the robot moves beyond the current submap's bounds or the submap contains excessive keyframes, a new submap is created. The new submap centers on the current frame with $\bm{T}_{WS} = \bm{T}_{WC}$, initially empty. For matching convenience, recent keyframes from the old submap are copied to the new one.  
	\item Merging all submaps' grid maps yields the global map.  
\end{enumerate}  

This workflow is not limited to 2D LiDAR SLAM. The submap paradigm can also be applied to 3D LiDAR or visual SLAM systems, though implementation becomes more complex than single-map approaches.

\subsection{Implementation of Submaps}  
At the implementation level, we encapsulate both grid map and likelihood field objects within the `Submap` class, while placing the mapping algorithm workflow in the `mapping_2d` class.  

The core structure of the `Submap` class is as follows:  
\begin{lstlisting}[language=c++,caption=src/ch6/submap.h]
class Submap {
	public:
	Submap(const SE2& pose) : pose_(pose) {
		Vec2f center = pose_.translation().cast<float>();
		occu_map_.SetCenter(center);
		field_.SetCenter(center);
	}
	
	/// Match frame against this submap to compute frame->pose
	bool MatchScan(std::shared_ptr<Frame> frame);
	
	/// Add a frame to the occupancy grid
	void AddScanInOccupancyMap(std::shared_ptr<Frame> frame);
	
	void AddKeyFrame(std::shared_ptr<Frame> frame) { frames_.emplace_back(frame); }
	
	private:
	SE2 pose_;  // submap pose, Tws
	size_t id_ = 0;
	
	std::vector<std::shared_ptr<Frame>> frames_;  // keyframes in this submap
	LikelihoodField field_;                       // for scan matching
	OccupancyMap occu_map_;                       // for grid map generation
};
\end{lstlisting}

The submap's primary functions are scan matching and grid map updates, implemented through internal object calls:  

\begin{lstlisting}[language=c++,caption=src/ch6/submap.cc]
bool Submap::MatchScan(std::shared_ptr<Frame> frame) {
	field_.SetSourceScan(frame->scan_);
	field_.AlignG2O(frame->pose_submap_);
	frame->pose_ = pose_ * frame->pose_submap_;  // T_w_c = T_w_s * T_s_c
	
	return true;
}

void Submap::AddScanInOccupancyMap(std::shared_ptr<Frame> frame) {
	occu_map_.AddLidarFrame(frame, OccupancyMap::GridMethod::MODEL_POINTS);  // update grid cells
	field_.SetFieldImageFromOccuMap(occu_map_.GetOccupancyGrid());           // update likelihood field
}
\end{lstlisting}

We employ g2o-based likelihood field methods for registration, incorporating boundary checks and robust kernels to mitigate moving object interference. Readers may refer to the source code for implementation details. The outer mapping workflow proceeds as follows:  

\begin{lstlisting}[language=c++,caption=src/ch6/mapping\_2d.cc]
bool Mapping2D::ProcessScan(Scan2d::Ptr scan) {
	current_frame_ = std::make_shared<Frame>(scan);
	current_frame_->id_ = frame_id_++;
	
	LOG(INFO) << "processing frame " << current_frame_->id_;
	if (last_frame_) {
		// initialize pose from last frame
		current_frame_->pose_ = last_frame_->pose_;
	}
	
	// perform scan-to-map matching
	if (!first_scan_) {
		// skip matching for first scan (directly add to occupancy map)
		current_submap_->MatchScan(current_frame_);
	}
	
	first_scan_ = false;
	current_submap_->AddScanInOccupancyMap(current_frame_);
	
	if (IsKeyFrame()) {
		AddKeyFrame();
		
		if (current_submap_->HasOutsidePoints() || (current_submap_->NumFrames()) > 50) {
			/// Trigger new submap when leaving current bounds or exceeding keyframe limit
			ExpandSubmap();
		}
	}
	
	last_frame_ = current_frame_;
	
	return true;
}

bool Mapping2D::IsKeyFrame() {
	if (last_keyframe_ == nullptr) {
		return true;
	}
	
	SE2 delta_pose = last_keyframe_->pose_.inverse() * current_frame_->pose_;
	if (delta_pose.translation().norm() > keyframe_pos_th_ || 
	fabs(delta_pose.so2().log()) > keyframe_ang_th_) {
		return true;
	}
	
	return false;
}

void Mapping2D::AddKeyFrame() {
	LOG(INFO) << "add keyframe " << keyframe_id_;
	current_frame_->keyframe_id_ = keyframe_id_++;
	
	current_submap_->AddKeyFrame(current_frame_);
	last_keyframe_ = current_frame_;
}

void Mapping2D::ExpandSubmap() {
	// archive current submap and create new one
	all_submaps_.emplace_back(current_submap_);
	
	current_submap_ = std::make_shared<Submap>(current_frame_->pose_);
	current_submap_->SetId(submap_id_++);
	current_submap_->AddKeyFrame(current_frame_);
	current_submap_->AddScanInOccupancyMap(current_frame_);
	
	LOG(INFO) << "create submap " << current_submap_->GetId();
}
\end{lstlisting}

The system continuously matches current scans against the active submap. When the robot moves beyond threshold distances, new keyframes are created and added to the current submap. Excessive keyframes trigger new submap creation, with old submaps archived for global mapping.

The test program for this section only needs to read LiDAR data from ROS bags, requiring no additional inputs. This completes our pure LiDAR-based 2D SLAM system:
\begin{lstlisting}[language=c++,caption=src/ch6/test_2d_mapping.cc]
	sad::RosbagIO rosbag_io(fLS::FLAGS_bag_path);
	sad::Mapping2D mapping;
	
	if (mapping.Init(FLAGS_with_loop_closing) == false) {
		return -1;
	}
	
	rosbag_io.AddScan2DHandle("/pavo_scan_bottom", [&](Scan2d::Ptr scan) { return mapping.ProcessScan(scan); }).Go();
	cv::imwrite("./data/ch6/global_map.png", mapping.ShowGlobalMap(1000));
	return 0;
\end{lstlisting}

Readers can run the \texttt{test\_2d\_mapping} program to observe the submap switching process along with each submap's grid and likelihood field visualization. By default, loop closure detection is disabled in this section. The next section will use the same test program with loop closure enabled to demonstrate its effects.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/submap-and-field.png}
	\caption{A single submap and its corresponding likelihood field in 2D LiDAR SLAM}
	\label{fig:submap-and-field}
\end{figure}

Figure~\ref{fig:submap-and-field}~shows an active submap and its likelihood field during operation. During testing, these visualizations update dynamically, allowing readers to observe both scan matching and submap transitions. The current LiDAR pose and scan data are overlaid in contrasting colors for clearer real-time localization and mapping visualization.

In addition to the active submap, the program outputs a global map image for debugging purposes (Figure~\ref{fig:global-map-without-loop-closing}). The actual image dimensions are configurable - readers may specify larger sizes for higher-resolution outputs. Since we haven't yet implemented loop closure, accumulated drift creates visible ghosting effects when the robot revisits areas. The next section will address this through loop detection algorithms to eliminate such cumulative errors.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.4\textwidth]{resources/2d-lidar-mapping/global-map-without-loop-closing.png}
	\caption{Global map without loop closure. When the robot returns from upper regions to the central area, noticeable ghosting appears between submaps.}
	\label{fig:global-map-without-loop-closing}
\end{figure}

\section{Loop Closure Detection and Correction}
Loop closure detection constitutes a critical component in SLAM systems. Without it, both LiDAR odometry and wheel-inertial odometry methods inevitably accumulate drift over time. The previous section's example clearly demonstrates this issue - when the robot completes a loop and returns to previously mapped areas, new submaps exhibit noticeable misalignment with historical ones. This naturally suggests that aligning current scans/submaps with historical maps and adjusting inter-submap pose relationships could effectively eliminate accumulated errors. While theoretically sound, several practical challenges emerge:

\begin{enumerate}
	\item \textbf{Detection Scope}: Which historical submaps should be examined? This defines the loop \textbf{detection} problem. Intuitively, submaps spatially proximate to current scans should undergo alignment. However, spatial relationships rely on estimated trajectories that may contain significant drift, potentially causing missed detections. Effective implementation requires approximate bounds on accumulated error magnitude.
	
	\item \textbf{Registration Methods}: Loop closure registration differs fundamentally from odometric alignment. Odometry assumes \textbf{continuous motion} with small displacements between optimizations. Loop closure initial guesses, however, depend on \textbf{accumulated error magnitude} and may deviate significantly from optimal alignments. Standard ICP and likelihood field methods suffer from strong initial value dependence. Practical solutions incorporate:
	\begin{itemize}
		\item Grid searching \cite{Scherer2013}
		\item Particle filters \cite{Stachniss2005} 
		\item Branch-and-bound (BAB) \cite{Hess2016}
		\item Image pyramids \cite{Qianhao2018}
	\end{itemize}
	Among these, branch-and-bound and pyramid approaches offer particularly robust performance with similar underlying principles.
	
	\item \textbf{Map Correction}: Upon loop detection, global pose adjustment can operate on either keyframes or submaps. Submap-based optimization benefits from significantly smaller problem dimensions (fewer submaps than keyframes), making it ideal when the frontend already employs submap management. However, submap-level correction cannot address intra-submap distortions, motivating some systems to retain keyframe-level optimization.
\end{enumerate}

\subsection{Multi-Resolution Loop Closure Detection}  
We now introduce the pyramid-based loop closure detection method, also known as \textbf{coarse-to-fine} or multi-resolution registration. Rather than being a single algorithm, this represents a general strategy for addressing initialization problems - an approach widely applicable beyond point cloud registration. For instance:  
- Branch-and-bound serves both as a coarse registration method and a fundamental technique in integer programming  
- Pyramid methods are equally vital for likelihood field registration and optical flow computation \cite{Fortun2015}  

From a search perspective, these methods efficiently explore solution spaces while maintaining optimality. Since both branch-and-bound and coarse-to-fine approaches utilize multi-resolution grid maps, we classify them collectively as multi-resolution loop closure techniques.  

This section focuses on multi-resolution likelihood field matching. While fundamentally still a scan matching problem, it specifically addresses scenarios with poor initial pose estimates. Our implementation extends the standard likelihood field (20 pixels/meter resolution) with additional lower-resolution fields. As shown in Figure~\ref{fig:multi-level-search}, we employ four field layers where each higher level halves the image dimensions. The coarsest level's blurred obstacle representations permit larger initial pose errors while maintaining discriminative power.  

\begin{figure}[!t]  
	\centering  
	\includegraphics[width=0.8\textwidth]{resources/2d-lidar-mapping/multi-level-search.png}  
	\caption{Multi-resolution registration visualization. Left: Original resolution field; Right: Downsampled fields. Red: Unaligned scans; Green: Registered scans.}  
	\label{fig:multi-level-search}  
\end{figure}  

The implementation (src/ch6/multi\_resolution\_likelihood\_field.cc) follows standard frame-to-frame matching but with adjusted parameters and multiple field layers:  

\begin{lstlisting}[language=c++,caption=src/ch6/multi_resolution_likelihood_field.cc]
	void MRLikelihoodField::BuildModel() {
		const int range = 20;  // Template pixel radius
		
		/// Pyramid field construction
		field_ = {
			cv::Mat(125, 125, CV_32F, 30.0),  // Level 0 (coarsest)
			cv::Mat(250, 250, CV_32F, 30.0),
			cv::Mat(500, 500, CV_32F, 30.0),
			cv::Mat(1000, 1000, CV_32F, 30.0), // Level 3 (finest)
		};
		
		// Template generation
		for (int x = -range; x <= range; ++x) {
			for (int y = -range; y <= range; ++y) {
				model_.emplace_back(x, y, std::sqrt((x * x) + (y * y)));
			}
		}
	}
	
	bool MRLikelihoodField::AlignG2O(SE2& init_pose) {
		num_inliers_.clear();
		inlier_ratio_.clear();
		
		// Hierarchical registration
		for (int l = 0; l < levels_; ++l) {
			if (!AlignInLevel(l, init_pose)) {
				return false;  // Abort if any level fails
			}
		}
		return true;
	}
	
	bool MRLikelihoodField::AlignInLevel(int level, SE2& init_pose) {
		// G2O optimization setup (omitted for brevity)
		...
		
		// Adaptive parameters
		const double rk_delta[] = {0.2, 0.3, 0.6, 0.8}; // Robust kernel thresholds
		const float inlier_ratio_th = 0.4;  // Minimum inlier percentage
		
		// Edge construction and optimization
		...
		
		// Validation
		if (num_inliers > 100 && inlier_ratio > inlier_ratio_th) {
			init_pose = v->estimate();
			return true;
		}
		return false;
	}
\end{lstlisting}

The pyramid matching proceeds sequentially - failure at any level terminates the process. Notably, loop closure scenarios often exhibit partial matches due to:  

\begin{enumerate}
\item  Significant initial pose errors  
\item  Limited LiDAR FOV (270° in our experiments)  
\item  Non-identical revisit poses 
\end{enumerate}
 

We therefore set a conservative inlier threshold (40\%) to balance sensitivity and robustness. This critical parameter trades off between:  
- \textbf{Strictness}: Higher thresholds require precise pose recurrence  
- \textbf{Sensitivity}: Lower thresholds permit more detections but increase false positives  

While we demonstrate coarse-to-fine registration, other methods like branch-and-bound \cite{Hess2016} similarly leverage multi-resolution maps. Both approaches share core principles but differ in implementation details - the former performs full optimization at each level, while the latter evaluates discrete candidates. Given their conceptual similarity, we focus on one representative implementation.

\subsection{Submap-Based Loop Closure Correction}  
If loop closure detection successfully establishes the registration relationship between the current frame and a historical submap, we initiate a loop closure correction. This problem can again be modeled as a pose graph problem on $\mathrm{SE}(2)$. Moreover, since we have already constructed submaps earlier, the pose graph problem can utilize only submap poses as optimization nodes.

Consider the relationship between the current frame and a historical submap. We compute the pose of the current frame within the historical submap through multi-resolution matching. Let the pose of the historical submap $S_1$ itself be $\bm{T}_{W S_1}$, the pose of the current frame be $\bm{T}_{WC}$, and the pose of the submap $S_2$ containing the current frame be $\bm{T}_{W S_2}$. The multi-resolution matching actually computes the relative pose $\bm{T}_{S_1 C}$. Thus, we can convert this result into the pose transformation between the historical submap and the current submap:

\begin{equation}\label{key}
	\bm{T}_{S_1 S_2} = \bm{T}_{S_1 C} \bm{T}_{WC}^{-1} \bm{T}_{W S_2}
\end{equation}
Thereby obtaining the relative pose relationship between $S_1$ and $S_2$. This process is illustrated in Figure~\ref{fig:submap-pose-graph}.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/2d-lidar-mapping/submap-pose-graph.pdf}
	\caption{Schematic diagram of submap pose graph optimization.}
	\label{fig:submap-pose-graph}
\end{figure}

In loop closure optimization, we treat each submap pose as an optimization variable and construct a pose graph for optimization. The observations in this pose graph primarily come from two sources: first, the relative pose observations between adjacent submaps, and second, the relative pose relationships between two submaps computed by loop closure detection. Again considering the relative pose between submap 1 and submap 2, assuming the relative pose observation computed by loop closure detection is $\bm{T}_{S_1 S_2}$, its residual term is:
\begin{equation}\label{key}
	\bm{e} = \mathrm{Log}( \bm{T}_{W S_1}^{-1} \bm{T}_{W S_2} \bm{T}_{S_1 S_2}^{-1}) \in \mathbb{R}^3.
\end{equation}

Its Jacobian matrix is rather cumbersome, so we leave it to automatic differentiation. To prevent incorrect loop closures, we also add a loop closure verification process: the accumulated error after correction should not be too large, otherwise the loop closure will be rejected as an outlier. The implementation code for loop closure detection is as follows:

\begin{lstlisting}[language=c++,caption=src/ch6/loop_closing.cc]
class LoopClosing {
	public:
	/// A loop closure constraint
	struct LoopConstraints {
		LoopConstraints(size_t id1, size_t id2, const SE2& T12) : id_submap1_(id1), id_submap2_(id2), T12_(T12) {}
		size_t id_submap1_ = 0;
		size_t id_submap2_ = 0;
		SE2 T12_;  //  Relative pose
		bool valid_ = true;
	};
	
	/// Add the most recent submap, which may still be under construction
	void AddNewSubmap(std::shared_ptr<Submap> submap);
	
	/// Add a completed submap
	void AddFinishedSubmap(std::shared_ptr<Submap> submap);
	
	/// Perform loop closure detection for a new frame and update its pose and submap poses
	void AddNewFrame(std::shared_ptr<Frame> frame);
	
	private:
	/// Detect possible loop closures between the current frame and historical maps
	bool DetectLoopCandidates();
	
	/// Match the current frame with historical submaps
	void MatchInHistorySubmaps();
	
	/// Perform pose graph optimization between submaps
	void Optimize();
	
	std::shared_ptr<Frame> current_frame_ = nullptr;
	size_t last_submap_id_ = 0;  // ID of the most recent submap
	
	std::map<size_t, std::shared_ptr<Submap>> submaps_;  // All submaps
	
	// Mapping between submaps and their multi-resolution fields
	std::map<std::shared_ptr<Submap>, std::shared_ptr<MRLikelihoodField>> submap_to_field_;
	
	std::vector<size_t> current_candidates_;                                 // Potential loop closure candidates
	std::map<std::pair<size_t, size_t>, LoopConstraints> loop_constraints_;  // Loop constraints indexed by constrained submap pairs
	
	/// Parameters
	inline static constexpr float candidate_distance_th_ = 15.0;  // Distance threshold between candidate frame and submap center
	inline static constexpr int submap_gap_ = 1;                  // Minimum submap index difference for loop closure
	inline static constexpr double loop_rk_delta_ = 1.0;          // Robust kernel threshold for loop closure
};

void LoopClosing::AddFinishedSubmap(std::shared_ptr<Submap> submap) {
	auto mr_field = std::make_shared<MRLikelihoodField>();
	mr_field->SetPose(submap->GetPose());
	mr_field->SetFieldImageFromOccuMap(submap->GetOccuMap().GetOccupancyGrid());
	submap_to_field_.emplace(submap, mr_field);
}

void LoopClosing::AddNewSubmap(std::shared_ptr<Submap> submap) {
	submaps_.emplace(submap->GetId(), submap);
	last_submap_id_ = submap->GetId();
}

void LoopClosing::AddNewFrame(std::shared_ptr<Frame> frame) {
	current_frame_ = frame;
	if (!DetectLoopCandidates()) {
		return;
	}
	
	MatchInHistorySubmaps();
	
	if (has_new_loops_) {
		Optimize();
	}
}

bool LoopClosing::DetectLoopCandidates() {
	// Require sufficient separation between current frame and historical submaps
	has_new_loops_ = false;
	if (last_submap_id_ < submap_gap_) {
		return false;
	}
	
	current_candidates_.clear();
	
	for (auto& sp : submaps_) {
		if ((last_submap_id_ - sp.first) <= submap_gap_) {
			// Skip recently created submaps
			continue;
		}
		
		// Skip if valid constraint already exists between these submaps
		auto hist_iter = loop_constraints_.find(std::pair<size_t, size_t>(sp.first, last_submap_id_));
		if (hist_iter != loop_constraints_.end() && hist_iter->second.valid_) {
			continue;
		}
		
		Vec2d center = sp.second->GetPose().translation();
		Vec2d frame_pos = current_frame_->pose_.translation();
		double dis = (center - frame_pos).norm();
		if (dis < candidate_distance_th_) {
			current_candidates_.emplace_back(sp.first);
		}
	}
	
	return !current_candidates_.empty();
}

void LoopClosing::MatchInHistorySubmaps() {
	// First save the scan, pose and submap to offline files for MR matching
	// current_frame_->Dump("./data/ch6/frame_" + std::to_string(current_frame_->id_) + ".txt");
	
	for (const size_t& can : current_candidates_) {
		auto mr = submap_to_field_.at(submaps_[can]);
		mr->SetSourceScan(current_frame_->scan_);
		
		auto submap = submaps_[can];
		SE2 pose_in_target_submap = submap->GetPose().inverse() * current_frame_->pose_;  // T_S1_C
		SE2 init_guess = pose_in_target_submap;
		
		if (mr->AlignG2O(pose_in_target_submap)) {
			// Set constraints from current submap to target submap
			// T_S1_S2 = T_S1_C * T_C_W * T_W_S2
			SE2 T_this_cur =
			pose_in_target_submap * current_frame_->pose_.inverse() * submaps_[last_submap_id_]->GetPose();
			loop_constraints_.emplace(std::pair<size_t, size_t>(can, last_submap_id_),
			LoopConstraints(can, last_submap_id_, T_this_cur));
			has_new_loops_ = true;
		}
	}
	
	current_candidates_.clear();
}
\end{lstlisting}

This code constructs a multi-resolution likelihood field for each submap upon completion, then matches recent scans with completed submaps. If multi-resolution matching succeeds, it calls the Optimize function to perform loop closure correction. The correction results directly affect each submap's pose. The Optimize function implements g2o graph optimization setup, which is similar to previously listed g2o programs and thus omitted here. To simplify the implementation, we do not run loop closure detection as a separate thread (which would require lock management for many data structures), but instead execute it sequentially in the main 2D mapping function. Meanwhile, we display submap coordinate systems and their loop closure relationships on the global map. Running the test\_2d\_mapping program with --with\_loop\_closing=true shows this process in real-time. The global map after loop closure correction is shown in Figure~\ref{fig:global-map}, where compared with the earlier Figure~\ref{fig:global-map-without-loop-closing}, clear improvements can be seen in the central section.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/2d-lidar-mapping/global-map.png}
	\caption{Global map with loop closure detection and inter-submap loop constraints.}
	\label{fig:global-map}
\end{figure}

Thus, we have completed a relatively comprehensive 2D LiDAR-only SLAM program based on a submap management framework. Such grid maps can be directly used for localization and navigation if saved. Many real-world robots or functionally simple autonomous vehicles achieve self-localization and navigation through similar approaches. However, for brevity, this implementation omits many engineering details that readers may further refine.

\subsection{Discussion}  
Below we present several potential improvements to the programs in this section. As this book primarily focuses on algorithmic principles and aims to avoid excessive engineering details that could complicate the code, the following content is mainly narrative in nature.

\subsubsection{Laser Motion Compensation, Reflectivity Issues, etc.}  
First, let us examine the laser sensor itself. A laser sensor rotates periodically and is not designed to account for the simultaneous rotation of the robot's chassis. If the chassis is stationary, the laser sensor should physically rotate 360 degrees when completing a full scan. However, if the robot itself is also rotating, the actual rotation angle when the laser completes a scan may be slightly more or less than 360 degrees. This deviation depends on the speed and direction of the chassis's rotation. Similarly, translational motion of the chassis affects the actual measured distance of each laser point. This phenomenon is referred to as \textbf{motion distortion}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\textwidth]{resources/2d-lidar-mapping/lidar-problems.pdf}
	\caption{Common issues in 2D LiDAR SLAM. Top-left: Without motion compensation, the laser scan covers slightly less than the actual scene; Top-right: When observing glass surfaces, perspective effects may incorrectly mark glass walls as passable areas; Bottom-left: Reflective artifacts at the end of a left wall create a phantom wall segment beyond the actual corridor; Bottom-right: Robot vibrations cause the laser to hit the ground, creating non-existent obstacles.}
	\label{fig:lidar-problems}
\end{figure}

Motion distortion can be mitigated using motion compensation algorithms. The basic idea is to acquire the robot's pose at the start and end of each laser scan cycle. Most laser sensors complete a full rotation in 100 milliseconds, so the robot's motion during this period must be accounted for by compensating each laser point. The same issue arises with 3D LiDAR sensors, which will be discussed in the next chapter. However, how do we obtain the poses at the start and end of the scan? After all, when the laser data is first received, the robot's pose has not yet been estimated, while motion compensation relies on this very estimate. In scenarios with an IMU, we can use its data to predict the robot's motion within the 100-millisecond window and thereby compensate for the distortion.

Motion compensation improves the accuracy of submap registration. We will detail the motion compensation algorithm in the chapter on 3D LiDAR SLAM (see Section~\ref{sec:motion-compensation}), as modern 3D LiDARs typically provide timestamps for each point, whereas 2D LiDARs often do not, requiring manual estimation based on the scanning process. This involves additional sensor-specific parameters, complicating the implementation.

On the other hand, LiDAR sensors measure distance based on the time difference between emitted and reflected light. If the target object absorbs, transmits, or specularly reflects the laser beam, the measured distance may be affected. A typical example is glass surfaces, which may partially reflect or transmit the laser, resulting in intermittent obstacles or walls on the map. Mirrors represent another extreme case, where the laser beam is reflected to another area, causing the measured distance in that direction to appear much longer and making the mirror itself undetectable on the map.

If the robot vibrates during motion, the LiDAR sensor may deviate from the horizontal plane, reducing measurement consistency. The laser may also hit the ground or slopes, violating the assumptions of the grid map model. Many robots require manual annotation of sloped areas in 2D SLAM. With an IMU, the robot's 3D orientation can be estimated to determine whether the laser is tilted.

The aforementioned issues can all be observed in the experiments of this section, as shown in Figure~\ref{fig:lidar-problems}. Therefore, real-world SLAM systems must address significantly more challenges than those covered in theoretical discussions.

\subsubsection{IMU and Robot Odometry Fusion}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.7\textwidth]{resources/2d-lidar-mapping/bad-cases.pdf}
	\caption{Degeneration issues in 2D LiDAR under degenerate and open scenarios.}
	\label{fig:bad-cases}
\end{figure}

The 2D SLAM introduced in this section is a pure LiDAR solution. Although LiDAR offers high measurement accuracy, single-line LiDAR has limited observation modes and typically narrow field of view, making pure LiDAR solutions susceptible to various environmental structures. Typical examples include open areas and corridor scenarios. In these environments, 2D scan matching algorithms fundamentally cannot uniquely determine the laser scan's position, potentially exhibiting one or more additional degrees of freedom. We refer to such issues as \textbf{degeneracy problems}.

In degenerate scenarios, most pure LiDAR scan matching algorithms struggle to provide correct pose estimation. For instance, in corridor environments, likelihood field methods generate fields concentrated along both corridor walls. If the robot moves along the corridor, new scan data will appear to match perfectly with existing corridor walls. Consequently, the matching algorithm may falsely conclude the robot hasn't moved. Similar phenomena occur with ICP-type methods. Here, we say the robot's pose estimation gains \textbf{extra degrees of freedom (gauge freedom)} along the corridor direction, while the perpendicular direction remains constrained, resulting in 1 additional degree of freedom. In open square scenarios, neither translation nor rotation can be uniquely determined, yielding 3 additional degrees of freedom. Beyond corridors and squares, many regular, symmetric environments (perfect circles, single walls, multiple parallel walls, cylindrical surfaces, etc.) also exhibit degeneracy - readers should be able to identify other degenerate scenarios.

Degeneracy problems are particularly pronounced in pure LiDAR SLAM systems. Incorporating additional sensors can partially compensate for these effects. Later chapters will introduce multi-sensor fusion SLAM systems, most commonly combining IMU, wheel encoders, odometry or GPS information with LiDAR to achieve more robust performance. Both loosely-coupled and tightly-coupled approaches can compensate for LiDAR mapping or localization in degenerate scenarios.

Readers can experiment with other datasets provided in this chapter to verify the SLAM system's core functionality (see Figure~\ref{fig:other-results}). Further improvements can be achieved by implementing motion compensation, degeneracy detection, secondary echo detection, and multi-sensor fusion. However, 2D SLAM ultimately faces practical limitations and is primarily suitable for relatively simple indoor applications. The 3D SLAM introduced in the next chapter demonstrates superior performance in large-scale outdoor environments.

\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\textwidth]{resources/2d-lidar-mapping/other-results.pdf}
	\caption{Mapping results from additional datasets in this chapter.}
	\label{fig:other-results}
\end{figure}

\section{Summary}
This chapter has presented various algorithmic modules for 2D LiDAR SLAM, which historically formed the core of SLAM research. In 2D scenarios, many problems can be simplified. We implemented registration algorithms including ICP, ICL and Gaussian fields, managed them using a submap framework, and ultimately constructed global grid maps. We also explained submap management across timestamps and loop closure correction for accumulated errors. The 2D SLAM system described here serves as a prototype for various single-line LiDAR mapping and localization applications.

\section*{Exercises}
\begin{enumerate}
	\item Implement point-to-point 2D ICP using an optimization framework (based on g2o or ceres).
	\item Implement point-to-line 2D ICP using an optimization framework (based on g2o or ceres).
	\item Implement likelihood field registration method using an optimization framework (based on g2o or ceres).
	\item In the likelihood field method, interpolation of the likelihood field image can be used to obtain more accurate error values and gradient functions. Implement a likelihood field scan matching method using linear interpolation.
	\item Implement a degeneracy detection method for 2D LiDAR using line fitting.
\end{enumerate}
